\chapter{Machine Learning}
\label{ch:digital_filters}

\section{Classification}
\label{sec:classification}

\subsection{Classification Basics}

The detector is required to distinguish smoke and steam from solid objects and to adjust its PWM output according to the smoke intensity. The key challenge here is to reliable distinguish between the three scenarios, that is smoke, no smoke, and moving solid objects, such as hands while cleaning, pots, insects, etc. Nevertheless, for the classification itself there are only two classes, that is 'smoke', which should switch on the extraction hood, and 'no smoke', which should switch off the extraction hood or, as the case may be, leave it switched off, regardless of any movement of solid objects within the detection area. The challenge originates from the fact that a solid object moving with a certain frequency in a certain distance might generate the same course of signals over time as smoke or steam closer to the detector. Once the classification works reliably one can determine additional features that can be used to adjust the detector output and therefore the speed of the fan.

\subsection{Support Vector Machine}

This section gives an introduction to the basics of an extremely powerful and widely utilised machine learning technique known as the \emph{support vector machine} (SVM), a discriminative classifier formally defined by a separating hyperplane. This involves the derivation of the mathematical formulation from the ground up, starting with the \emph{maximal margin classifier}, extending it to the \emph{support vector classifier} and finally attaining the \emph{support vector machine}. Given a set of training examples, each previously assigned to one of two categories, $c_1$ and $c_2$, also referred to as \emph{classes}, a SVM training algorithm builds a model that assigns new examples to one of the two categories. This makes it a non-probabilistic, binary, linear classifier. In case the classification problem is non-linear, that is, the two classes cannot be divided in two by a linear hyperplane, a SVM can efficiently perform a non-linear classification using the kernel trick, which refers to implicitly mapping its input into a higher-dimensional feature space.

\subsubsection{Feature Space}
In machine learning, a feature space is a finite-dimensional vector space spanned by the feature vectors,

\begin{equation}
   \bm{x} = (x_1, \dots, x_p) \in \mathbb{R}^p\,.   
\end{equation}
   
\noindent
Each of the components $x_1, \dots, x_p$ of the feature vector, referred to as features, represent an individual measurable property of a phenomenon being observed. The feature vector for our classification model is constituted of $p$ discrete frequency components of the signal resulting from a fast Fourier transform (FFT) with a block length of $m$ samples.

A block refers to a set of $m$ uniformly spaced samples of the loop signal in the time domain. The size of the feature space is related to the block length of the FFT according to 

\begin{equation}
p = m / 2 + 1, \quad m = 2^k, \quad k \in \mathbb{Z}, \quad 2 \leq k \leq 10\,.
\label{eq:space_dim}
\end{equation}

\noindent
Thus, the $p$ features represent the normalised single-sided amplitude spectrum of the signal. The first feature, given by $x_1$, represents the steady component of the loop signal being transformed, whereas the remaining $x_2, \dots, x_p$ features represent the discrete frequency components of the signal \cite{smith1997scientist}. These discrete frequencies are integer multiples of the fundamental frequency

\begin{equation}
\Delta f = \frac{f_s}{m} = \frac{1}{m T_s}\,.
\end{equation}

\noindent
Its corresponding period is defined by the temporal extent of one block of $m$ samples, which means that a greater block length will lead to a higher frequency resolution and therefore to a greater feature space. As the computational complexity will increase with both an increasing block length of the FFT and a greater feature space the maximum size is limited and depends on the available hardware. Nevertheless, since only a subset of the training points are used in the actual decision process of assigning new members, only these points need to be stored in memory and calculated upon when making decisions \cite{svm_beginners}. This makes the SVM very memory-efficient and thus particularly suitable for an implementation on an embedded device.

\subsubsection{Maximum-margin Hyperplane}

In the case of a support vector machine, a data point is represented by a $p$-dimensional vector, that is a list of $p$ features, and the goal is to  separate such points with a $(p-1)$-dimensional hyperplane. The term hyperplane designates a subspace of one dimension less than its ambient space. It divides the feature space into two subspaces, i.\,e. two regions. There may be many hyperplanes that classify the data, whereas the most reasonable choice of a separating hyperplane is the one with the largest margin between the two classes. In case such hyperplane exists, it is known as the \emph{maximum-margin hyperplane} and the linear classifier it defines is known as the \emph{maximum margin classifier}. To illustrate this, Figure \ref{fig:hyperplanes} depicts three different hyperplanes in two dimensions. H1 does not separate the classes, H2 does, but only with a small margin and H3 separates them with the maximum margin. For the sake of clarity, this and the following examples illustrate the matter for a two-dimensional feature space but can be extended to a features space of arbitrary size.



Mathematically, we can define a hyperplane $H$ as a set of points $\bm{x}$ in the $p$-dimensional feature space

\begin{equation}
H = \{ (x_1, \dots, x_p) \in \mathbb{R}^p \mid \omega_1 x_1 + \dots + \omega_p x_p - b = 0 \}\,,
\end{equation}

\noindent
or using a more succinct notation, a set of points satisfying

\begin{equation}
\sum_{j = 1}^{p} \omega_j x_j - b = \langle \bm{\omega} , \bm{x} \rangle - b = 0\,,
\label{eq:hyperplane}
\end{equation}

\noindent
where $\bm{\omega}$ denotes the not necessarily normalised normal vector to the hyperplane and $\langle \bm{\omega} , \bm{x} \rangle$ the inner product of $\bm{\omega}$ and $\bm{x}$. In this context the normal vector is also referred to as  the weight vector. The parameter $\frac{b}{\|\bm{\omega}\|}$ determines the distance from the origin, i.\,e. $\bm{x} = 0$, to the hyperplane along the normal vector $\bm{\omega}$. If an element $\bm{x}$ satisfies the relation in Equation \ref{eq:hyperplane} then it lives on the hyperplane, whereas when it lies above, it satisfies

\begin{equation}
\langle \bm{\omega} , \bm{x} \rangle - b > 0\,,
\end{equation}

\noindent
and when it lies below it satisfies
 
\begin{equation}
\langle \bm{\omega} , \bm{x} \rangle - b < 0\,.
\end{equation}

\noindent
Therefore, calculating the sign of the expression $\langle \bm{\omega} , \bm{x} \rangle - b$ enables us to determine which side of the plane any element $\bm{x}$ will fall on and so to assign a class label.

\subsubsection{Maximal Margin Classifier}

Given a linearly separable training set of $n$ pairs of the form

 \begin{equation}
\mathcal{T} = \big\{(\bm{x}_i, y_i) \mid y_i \in \{-1, 1\}, \quad i \in \{1, \dots, n\}\big\}\,, 
\end{equation}

\noindent
where $y_i$, denotes the class label, respectively, there are two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible, which is illustrated in Figure \ref{fig:max_hyperplanes}. Then, the \emph{maximum-margin hyperplane} is the hyperplane that lies halfway between them. The two hyperplanes that bound the margin are given by

\begin{equation}
\langle \bm{\omega} , \bm{x} \rangle - b = 1
\end{equation}

\noindent
and

\begin{equation}
\langle \bm{\omega} , \bm{x} \rangle - b = -1\,.
\end{equation}

As the distance between these two hyperplanes is $\frac{2}{\|\bm{\omega}\|}$, and we want to maximize the distance between the planes we minimise $\|\bm{\omega}\|$, the Euclidean norm of the normal vector $\bm{\omega}$. In order to prevent data points from falling into the margin and to make sure each data point lies on the correct side of the margin we add the following constraint:

\begin{equation}
\langle \bm{\omega} , \bm{x}_i \rangle - b \geq 1, \quad \mbox{if} \ y_i = 1\,,
\end{equation}

\begin{equation}
\langle \bm{\omega} , \bm{x}_i \rangle - b \leq 1, \quad \mbox{if} \ y_i = -1\,,
\end{equation}

\noindent
which can be written as

\begin{equation}
y_i (\langle \bm{\omega} , \bm{x}_i \rangle - b) \geq 1, \quad \forall \ 1 \leq i \leq n\,.
\label{eq:constraint}
\end{equation}

\noindent
From these equation one can derive the following optimisation problem:

\begin{quote}
``Minimise $\|\bm{\omega}\|$ subject to $y_i (\langle \bm{\omega} , \bm{x}_i \rangle - b) \geq 1, \quad \forall \ 1 \leq i \leq n$ by varying $\bm{\omega}$ and $b$''
\end{quote}

\noindent
The $\bm{\omega}$ and $b$ that solve this problem determine the maximimal margin classifier

\begin{equation}
\begin{split}
f\colon\, \mathbb{R}^p & \to \{-1, 1\} \\
\bm{x} & \mapsto y \\
\bm{x} & \mapsto \mbox{sgn}(\langle \bm{\omega} , \bm{x} \rangle - b)\,.
\end{split}
\end{equation}

\noindent
Finding the solution to this optimisation problem is also referred to as \emph{training} the classifier. One of the key features of the maximal margin classifier, as well as the support vector classifier and the support vector machine that will be derived in the following sections is that the maximum-margin hyperplane is completely determined by those  $\bm{x}_i$ which lie directly on the margin boundary. These $\bm{x}_i$ are called \emph{support vectors}.

\subsubsection{Support Vector Classifier}

To extend the maximal margin classifier to cases in which the data are not linearly separable, that is, it is not possible to construct a separating hyperplane without encountering classification errors, we relax the requirement that a separating hyperplane will perfectly separate every training observation by using what is called a \emph{soft margin}.


\noindent
The margin of separation between the two classes is said to be soft if a data point $(\bm{x}_i, y_i)$ violates the condition in Equation \ref{eq:constraint}. This violation can arise in one of two ways, from \cite{haykin2009neural}:

\begin{itemize}

\item Correct classification: the data point  $(\bm{x}_i, y_i)$ falls inside the region of separation, but on the correct side of the decision surface, as illustrated in Figure \ref{fig:soft_margin}\,a.

\item Misclassification: the data point $(\bm{x}_i, y_i)$ falls on the wrong side of the decision surface, as illustrated in Figure \ref{fig:soft_margin}\,b.

\end{itemize}


In order to model a soft margin we introduce the hinge loss function, given by

\begin{equation}
\max\left(0, 1 - y_i (\langle \bm{\omega} , \bm{x}_i \rangle - b) \right)\,,
\end{equation}

\noindent
which is zero if the constraint in Equation \ref{eq:constraint} is satisfied, that is if the data point lies on the correct side of the margin. For data on the wrong side of the margin, the function's value is proportional to the distance from the margin. The $\bm{\omega}$ and $b$ that minimise the cost functional

\begin{equation}
\Phi(\bm{\omega}, b) = \left[\frac{1}{n} \sum_{i=1}^n \max\left(0, 1 - y_i (\langle \bm{\omega} , \bm{x}_i \rangle - b) \right)\right] + \lambda \|\bm{\omega}\|^2
\label{eq:soft_marg}
\end{equation}

\noindent
determine the \emph{soft margin classifier}, also known as the \emph{support vector classifier}. The parameter $\lambda$ determines the tradeoff between increasing the margin-size and ensuring that the data points $\bm{x}_i$ lie on the correct side of the margin. The support vector classifier will behave identically to the maximal margin classifier with its hard margin if the input data are linearly classifiable. If not, it will still learn a viable classification rule and thus avoids being overfitted.


\subsubsection{Kernel Trick}

Using a linear classification model may not work well if the underlying problem is highly non-linear. A possible solution is mapping the data into a higher-dimensional feature space that is hidden from both the input and output via some non-linear transformation

\begin{equation}
\bm{\varphi} \colon \mathbb{R}^{p} \to \mathbb{R}^{q}, \quad \bm{x} \mapsto \bm{\varphi}(\bm{x}), \quad p < q \,,
\end{equation}

\noindent
which increases the number of possible linear seperations accroding to Cover's Theorem \cite{cover}. This precedure is referred to as the \emph{kernel trick} \cite{haykin2009neural}. A \emph{support vector machine} can be considered an extension of a \emph{support vector classifier} that results from enlargening the feature space through the use of functions known as \emph{kernels}. In other words, it can learn a non-linear classification rule that corresponds to a linear classification rule for the transformed data points. This scenario is depicted in Figure \ref{fig:kernel_trick}, which illustrates the two mappings in a support vector machine for pattern classification: (i) the non-linear mapping from the input space to the feature space and (ii) the linear mapping from the feature space to the output space.




In order to formally take into account non-separable data points we rewrite the optimisation problem stated by Equation \ref{eq:soft_marg} as a constrained optimisation problem with a differentiable objective function. For that, we modify Equation \ref{eq:constraint} by introducing a new set of non-negative scalar \emph{slack variables},

 \begin{equation}
\{\xi_i \mid i \in \{1, \dots, n\}, \quad \xi_i > 0\}\,,
\end{equation}

\noindent
into the definition of the separating hyperplane, which leads to the following constraint:

\begin{equation}
y_i (\langle \bm{\omega} , \bm{x}_i \rangle - b) \geq 1 - \xi_i, \quad \forall \ 1 \leq i \leq n\,.
\label{eq:constraint_slack}
\end{equation}

\noindent
The slack variables represent the deviation of a data point from the ideal condition of pattern separability, respectively. As depicted in Figure \ref{fig:soft_margin}a, the data points fall inside the margin but on the correct side of the decision surface for $0 < \xi \leq 1$, whereas for $\xi > 1$ they fall on the wrong side of the separating hyperplane, as depicted in Figure \ref{fig:soft_margin}b.

As derived in detail in \cite{haykin2009neural}, the \emph{primal} optimisation problem arising from the above mentioned constraints is the following:

\begin{quote}

``Given the training samples $\mathcal{T}$ find the optimum values of the normal vector $\bm{\omega}$ and bias $b$ such that they satisfy the constraint

\begin{equation}
y_i (\langle \bm{\omega} , \bm{x}_i \rangle - b) \geq 1 - \xi_i, \quad \quad \xi_i > 0\ \quad  \forall i
\end{equation}

\noindent
and such that the weight vector $\bm{\omega}$ and the slack variables $\xi_i$ minimise the cost functional

\begin{equation}
\Phi(\bm{\omega}, \xi) = \frac{1}{2} \|\bm{\omega}\|^2 + \lambda \sum_{i=1}^n \xi_i \,,
\end{equation}

\noindent
where $\lambda$ is a user-specified positive parameter.''

\end{quote}

\noindent
The parameter $\lambda$ controls the tradeoff between complexity of the machine and the number of non-separable points. It may therefore be viewed as the reciprocal of a parameter commonly referred to as the ``regularisation'' parameter \cite{haykin2009neural}.

We may formulate the Lagrangian \emph{dual} problem expressing the normal vector of the hyperplane, $\bm{\omega}$, as a linear combination of the training examples

\begin{equation}
\bm{\omega} = \sum_{i=1}^n \alpha_i y_i \bm{x}_i \,,
\label{eq:linear_comb}
\end{equation}

\noindent
where the coefficients $\alpha_i$ are called Lagrange multipliers. Solving for the Lagrangian dual of the above problem yields the simplified optimisation problem, from \cite{haykin2009neural}:

\begin{quote}

``Given the training samples $\mathcal{T}$ find the Lagrange multipliers $\{\alpha_i \mid i \in \{1, \dots, n\}\}$ that maximise the objective function

\begin{equation}
Q(\alpha) =  \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j y_i y_j \langle \bm{x}_i , \bm{x}_j \rangle
\end{equation}

\noindent
subject to the constraints

\begin{equation}
\sum_{i=1}^n \alpha_i y_i = 0\,,
\end{equation}

\begin{equation}
 0 \leq \alpha_i \leq \lambda, \quad \forall \ 1 \leq i \leq n\,,
\end{equation}

\noindent
where $\lambda$ is a user-specified positive parameter.''

\end{quote}

\noindent
The offset $b$ can be recovered by finding an $\bm{x}_i$ on the margin's boundary and solving

\begin{equation}
y_i (\langle \bm{\omega} , \bm{x}_i \rangle + b) = 1 \Leftrightarrow b = y_i - \langle \bm{\omega} , \bm{x}_i \rangle\,.
\end{equation}

Now, to follow our objective of transforming the input feature space $\mathbb{R}^{p}$ to a higher-dimensional feature space, let 

 \begin{equation}
\big\{\varphi_j \mid j \in \{1, 2, 3, \dots \}\big\}\,, 
\end{equation}

\noindent
denote an infinitely large set of non-linear functions that transform the input space of dimension $p$ to a feature space of infinite dimensionality. Setting the bias $b$ in Equation \ref{eq:hyperplane} to zero for convenience of presentation, this transform yields

\begin{equation}
\sum_{j = 1}^{\infty} \omega_j \varphi_j(\bm{x}) = \langle \bm{\omega} , \bm{\varphi}(\bm{x}) \rangle = 0\,,
\label{eq:constrained_transformed}
\end{equation}

\noindent
where $\bm{\varphi}(\bm{x})$ denotes the transformed input vector $\bm{x}$ and is called the \emph{feature vector}. Adapting Equation \ref{eq:linear_comb} to the present situation we may write the weight vector as

\begin{equation}
\bm{\omega} = \sum_{i=1}^{n_s} \alpha_i y_i \bm{\varphi}(\bm{x}_i) \,,
\label{eq:linear_comb_transformed}
\end{equation}

\noindent
with

\begin{equation}
\bm{\varphi}(\bm{x}_i) = [\varphi_1(\bm{x}_i), \varphi_2(\bm{x}_i), \dots]^T\,,
\end{equation}

\noindent
where $n_s$ denotes the number of support vectors.

\noindent
Plugging Equation \ref{eq:linear_comb_transformed} into Equation \ref{eq:constrained_transformed} the decision surface in the output space can be expressed as 

\begin{equation}
\sum_{i=1}^{n_s} \alpha_i y_i \langle \bm{\varphi}(\bm{x}_i) , \bm{\varphi}(\bm{x}) \rangle = 0\ \,.
\end{equation}

\noindent
Let the inner product $\langle \bm{\varphi}(\bm{x}_i) , \bm{\varphi}(\bm{x}) \rangle$ be denoted as the scalar \emph{inner-product kernel}, or simply \emph{kernel}

\begin{equation}
\begin{split}
k(\bm{x}_i, \bm{x}) & = \langle \bm{\varphi}(\bm{x}_i) , \bm{\varphi} (\bm{x}) \rangle\ \\
& = \sum_{j = 1}^{\infty} \varphi_j(\bm{x_i}) \varphi_j(\bm{x}), \quad i \in \{1, 2, \dots n_s\}\,.
\end{split}
\end{equation}

\noindent
Then, the corresponding optimal decision surface in the output space can be expressed as 

\begin{equation}
\sum_{i=1}^{n_s} \alpha_i y_i k(\bm{x}_i, \bm{x}) = 0 \,.
\end{equation}

\noindent
This decision surface is non-linear in the input space, but its image in the feature space is linear.

Finally, replacing the inner product by the kernel $k(\bm{x}_i, \bm{x})$ results in the dual form for the constrained optimisation problem of a support vector machine as follows, from \cite{haykin2009neural}:

\begin{quote}

``Given the training samples $\mathcal{T}$ find the Lagrange multipliers $\{\alpha_i \mid i \in \{1, \dots, n\}\}$ that maximise the objective function

\begin{equation}
Q(\alpha) =  \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j y_i y_j k(\bm{x}_i, \bm{x})
\end{equation}

\noindent
subject to the constraints

\begin{equation}
\sum_{i=1}^n \alpha_i y_i = 0\,,
\end{equation}

\begin{equation}
 0 \leq \alpha_i \leq \lambda, \quad \forall \ 1 \leq i \leq n\,,
 \label{eq:lambda}
\end{equation}

\noindent
where $\lambda$ is a user-specified positive parameter.''

\end{quote}

\noindent
The resulting classifier has the form

\begin{equation}
\begin{split}
f(\bm{x}) & = \mbox{sgn}\big(\langle \bm{\omega} , \varphi(\bm{x}) \rangle - b\big) \\
& = \mbox{sgn}\bigg(\sum_{i=1}^{n_s} \alpha_i y_i k(\bm{x}_i, \bm{x}) - b\bigg) \,.
\end{split}
\label{eq:svm_classifier}
\end{equation}


\subsection{Cross-Validation}

Solving the optimisation problem given above aims at training the classifier so that it learns enough about the past to generalise to the future. In order to estimate how accurately our model will perform in practice we use cross validation. That is, we define a dataset to test the performance of the model in the training phase. By means of cross-validation we can limit overfitting and assess how the model will generalise to an independent data set. We partition a sample of data including their previously assigned class labels into two complementary subsets, one called the \emph{training set} and the other called the \emph{testing set}. Then, the classification model, i.\,e. the support vector machine, is trained using the training data set and validated using the testing data set. By performing multiple rounds of cross-validation, using different partitions and averaging the results, we derive a more accurate estimate of model prediction performance and therefore variability is reduced.

While there are many different types of cross-validation methods, all of which can be divided roughly into exhaustive and non-exhaustive cross-validation, we use the non-exhaustive $k$-fold cross-validation. The original sample $\mathcal{T}$ consisting of $n$ elements is randomly partitioned into $k < n$ roughly equally sized subsets $\mathcal{T}_1, \dots, \mathcal{T}_k$. Then, $k - 1$ subsets are used for training the model and the remaining subset is used for validating the model. The cross-validation process is then repeated $k$ times with each of the $k$ subsets used exactly once as the testing set. Finally, in order to produce a single estimation, the $k$ results from the folds are averaged or combined otherwise.


\subsection{\textsc{Matlab}\textsuperscript{\textregistered} Implementation}

The support vector machine derived above can be trained and validated using the \textsc{Matlab}\textsuperscript{\textregistered} script provided in Listing \ref{lis:matlab_code}. This script serves as a framework for the signal processing of the HALIOS\textsuperscript{\textregistered} loop signals and the final development and implementation of the classifier on an embedded device. This section presents a detailed description of the code below, where a grey box denotes \textsc{Matlab}\textsuperscript{\textregistered} variables and functions, or string literals when put in single quotation marks.

\begin{enumerate}
\item Specify numerous parameters:

\begin{itemize}

\item \code{power} is an integer that specifies the block length $m = 2^{\small\code{power}}$, of the FFT, with $2 \leq {\code{power}} \leq 10$. The block length is limited to $2^{10} = 1024$ samples as the optimised C-algorithm does not support a greater block length. This is due to the fact that the values of a sine wave are stored in the flash instead of being computed in real time. If the look-up table is adjusted, essentially any block length is possible. This parameter also specifies the dimension of the feature space for the classification according to Equation \ref{eq:space_dim}.

\item \code{sample\_period} is an integer that specifies the time between two samples in milliseconds. It can be modified when recording data with the HACo tool and is essential for correct results of the following computations.

\item \code{sensitivity} is a floating point number that varies the sensitivity of the detector on the edges between the two class labels `smoke' and 'no smoke' and is described in detail below.

\item \code{select\_segment} is a Boolean variable. If true the single-sided amplitude spectrum of a selected \code{block\_length}-wide segment of loop 0 training data is computed and  plotted. In addition to that the error of the result of the C-algorithm with respect to the result of the \textsc{Matlab}\textsuperscript{\textregistered} \code{fft} function is computed and illustrated in another plot. 

\item \code{file\_name\_train} is a string that specifies the file name of a .txt-file that contains the training data set. The data in this file are used for training and cross-validating the classifier. Training and test data must have the same sample frequency.

\item \code{file\_name\_test} is a string that specifies the file name of a .txt-file that contains the test data set. The data in this file represent independent data that were not used for the training of the classifier. Therefore, this data can be used for independent testing after the classifier was already trained and tuned to see how it behaves and how its overall performance is in practice. Training and test data must have the same sample frequency.

\item \code{training\_frac} is an integer that specifies in per cent how much of the training data set will be used for training the classifier. If $0 < $\code{training\_frac}$ < 100$ the first \code{training\_frac} per cent of the data set in \code{'file\_name\_train.txt'} will be used for training and the remaining data for testing. In this case \code{'file\_name\_test'}  is not used and can be left disregarded. If \code{training\_frac} equals 100 the data in \code{'file\_name\_train.txt'} will be used for training and the data in \code{'file\_name\_test.txt'} will be used for testing. This leaves the decision whether one or two files are used for training and testing to the user and is useful if only one long data set available. Also, this feature can be used to easily adjust the number of training samples. Training and test data must have the same sample frequency.

\item \code{kernel\_func} is a string that specifies the kernel function used in the support vector machine. Possible values are \code{'polynomial'} or \code{'gaussian'}. For further information on this and the two following name-value pair arguments please see \cite{fitcsvm}.

\item \code{poly\_order} is a positive integer that specifies the order of the polynomial kernel function used in the support vector machine. It is not necessary if a Gaussian kernel is specified.

\item \code{box\_constraint} is a positive integer that specifies the box constraint of the \textsc{Matlab}\textsuperscript{\textregistered} function \code{fitcsvm}, which trains the support vector model using the training data. It represents the parameter $\lambda$ in Equation \ref{eq:lambda}, introduced in the theoretical model above, and controls the maximum penalty imposed on margin-violating observations. Therefore it aids in preventing overfitting (regularisation). Increasing the box constraint leads to fewer assigned support vectors and can lead to longer training times.

\end{itemize}

Moreover, there are several boolean variables that control plotting. Either of these variables can be used to suppress certain plots that are not required for the development and tuning of the classifier. However, it is strongly recommended here not to suppress the plots and accept the somewhat longer processing time instead. The plots provided can help detecting irregularities in the data sets used for training an testing and provide validation of the SVM model and signal processing tasks carried out while training the classifier. In addition to the plots, there are several outputs in the console that inform the user about the parameters extracted from the text files and the results obtained from training and validating the support vector machine.

\end{enumerate}

\section{Results}

The results of the experiments are presented as follows:

\begin{itemize}
  \item \textsc{thigh angles:} Comparison of the temporal courses of the pitch angles of the right thigh with respect to the $x$-axis obtained by projection of the gravity vector, classical Kalman filtering, and extended Kalman filtering, in comparison to the reference, for the three different walking speeds, depicted in Figures \ref{fig:experiment_4}, \ref{fig:experiment_8}, and \ref{fig:experiment_12}.
  \item \textsc{shank angles:} Comparison of the temporal courses of the pitch angles of the right shank with respect to the $x$-axis obtained by projection of the gravity vector, classical Kalman filtering, and extended Kalman filtering, in comparison to the reference, for the three different walking speeds, depicted in Figures \ref{fig:experiment_5}, \ref{fig:experiment_9}, and \ref{fig:experiment_13}.
  \item \textsc{rmse comparison:} Root-mean-square error comparison of angle estimations by projection of the gravity vector, classical Kalman filtering, and extended Kalman filtering, for the three different walking speeds, depicted in Figures \ref{fig:experiment_6}, \ref{fig:experiment_10}, and \ref{fig:experiment_14}.
  \item \textsc{acceleration correction:} Comparison of the accelerometer-based shank angles with respect to the $x$-axis with and without correction of the acceleration signal, for the three different walking speeds, depicted in Figures \ref{fig:experiment_7}, \ref{fig:experiment_11}, and \ref{fig:experiment_15}.
\end{itemize}

\noindent
The entire absolute and relative RMSEs are summarised in Table \ref{tab:rmse}. This leads to an average improvement of the extended Kalman filter output with respect to the classical Kalman filter by a relative RMSE of $28.52\%$ and an average improvement of the motion-corrected accelerometer based angle estimate by a relative RMSE of $8.96\%$.

\section{Discussion}

After the presentation of the results, we now proceed to analysing them in detail. According to Table \ref{tab:rmse} and Figures \ref{fig:experiment_7} and \ref{fig:experiment_11}, the RMSEs of the motion-corrected accelerometer-based angle estimates while walking at $\unitfrac[2]{km}{h}$ and $\unitfrac[4]{km}{h}$ were only slightly different from the raw accelerometer-based angle estimates. This could be explained by the simple kinematic model of the leg. Even though it considers the motion of the leg in $x$ and $z$-direction with respect to the origin of the world coordinate frame, that is the hip joint, it does not consider motion of the entire system itself. However, while walking, the human body rotates about the hip joint, which causes motion of the respective opposite hip joint. Additionally, the tension of the calf muscles and the resulting stretching of the foot moves the entire body upwards, including the hip joint. That means that the origin of the world coordinate frame of the kinematic model moves, contrary to the assumption made in the model. This movement causes not only an additional acceleration component in the sensor signal while moving the leg in the air, but also a quite severe impact on the acceleration signal when touching the ground with the foot. Another fact that is not considered in the simple model is motion beyond the $xz$-plane. Moreover, the lengths of the links was unknown and thus only estimated. Finally, the sensor position on the thigh and shank was not exactly known because the sensors were attached to the leg with elastic straps while the subject wore cloths, but it was assumed that they are perfectly aligned with the thighs and shanks. As the trend of the relative RMSEs of the accelerometer-based angle estimates in the fourth row of Table \ref{tab:rmse} shows, the motion correction works better for faster motions, which indicates the importance of the correction at higher walking speeds. 

 Even though the acceleration correction does not benefit the RMSE of the accelerometer-based angle estimate for walking speeds of $\unitfrac[2]{km}{h}$ and $\unitfrac[4]{km}{h}$, the overall improvement of the filter output is significant for all three walking speeds, as stated in the fifth row of Table \ref{tab:rmse}. This improvement could be caused by a better filter tuning of the extended Kalman filter, and suboptimal tuning of the classical Kalman filter. The parameters found in an optimisation process, are not necessarily optimal. Due to the fact that the employed optimiser does not consider all possible combinations of the parameters, but instead finds local minima of the error function, the parameters may still be somewhat less than optimal and are strongly dependent on the initial guess. As one can see in Figures \ref{fig:experiment_7}, \ref{fig:experiment_11}, and \ref{fig:experiment_15}, the motion-based acceleration correction improves the angle estimate in some intervals, but even worsens it in others. The Kalman filter compensates those deviations trusting on the state-space model. The newly implemented filter algorithm is based on a more complete state-space model. For instance, it combines information about both thigh and shank in the same state-space model, whereas the classical Kalman filter estimates the thigh and shank angle independently. This could explain a more accurate estimate, compared to the classical Kalman filter, for intervals in which the acceleration correction does not decrease the RMSE of the accelerometer-based angle estimate.

